<div class="module-bar" data-module="processing">
    <button class="module-bar-title">suitkaise.processing</button>
    <nav class="module-bar-nav">
        <a href="#processing-why" class="module-bar-link active" data-page="processing-why">why</a>
        <a href="#processing-quick-start" class="module-bar-link" data-page="processing-quick-start">quick start</a>
        <a href="#processing" class="module-bar-link" data-page="processing">how to use</a>
        <a href="#processing-how-it-works" class="module-bar-link" data-page="processing-how-it-works">how it works</a>
        <a href="#processing-examples" class="module-bar-link" data-page="processing-examples">examples</a>
        <a href="#processing-videos" class="module-bar-link" data-page="processing-videos">videos</a>
        <a href="#processing-tests" class="module-bar-link" data-page="processing-tests">tests</a>
        <a href="#processing-learn" class="module-bar-link" data-page="processing-learn">learn</a>
    </nav>
</div>
<section class="module-page why-page">
    <h1>Why you should use <code>processing</code></h1>
    <h2>TLDR</h2>
    <ul>
        <li><strong>Anything works in parallel</strong> - Locally-defined functions, lambdas, closures, live connections all work</li>
        <li><strong>Easiest shared state possible</strong> - <code>share.counter = 0</code> just works across processes</li>
        <li><strong>Class-based processes</strong> - No more giant, messy functions. Lifecycle hooks organize your code naturally.</li>
        <li><strong>Crash and restart</strong> - <code>lives=3</code> and your process auto-retries. No try/except loops.</li>
        <li><strong>Timeouts</strong> - Advanced timeout system that works on all platforms.</li>
        <li><strong>Database connections just work</strong> - <code>@autoreconnect</code> brings live connections into subprocesses. Normally impossible.</li>
        <li><strong>Sync and async in one API</strong> - Same code, add <code>.asynced()</code> when you need it.</li>
    </ul>
    <hr>
    <h2>See it in action</h2>
    <p>Try sharing a logger, a list, and a counter across 4 parallel processes using standard <code>multiprocessing</code>:</p>
    <pre><code class="language-python">share.counter = 0
share.results = []
share.log = logging.getLogger(&quot;worker&quot;)</code></pre>
    <p>Just kidding, you can&#x27;t. <code>multiprocessing.Manager</code> doesn&#x27;t support loggers. <code>pickle</code> will choke. You&#x27;d need to redesign everything.</p>
    <p>Not with <code>processing</code>.</p>
    <pre><code class="language-python">from suitkaise.processing import Share, Pool, Skprocess
import logging

# put anything on Share — literally anything
share = Share()
share.counter = 0
share.results = []
share.log = logging.getLogger(&quot;worker&quot;)

class Worker(Skprocess):
    def __init__(self, share, item):
        self.share = share
        self.item = item

    def __run__(self):
        result = self.item * 2
        self.share.results.append(result)       # shared list
        self.share.counter += 1                 # shared counter
        self.share.log.info(f&quot;done: {result}&quot;)  # shared logger

pool = Pool(workers=4)
pool.star().map(Worker, [(share, x) for x in range(20)])

print(share.counter)         # 20
print(len(share.results))    # 20
print(share.log.handlers)    # still works</code></pre>
    <p>A list, a counter, and a logger — shared across 4 processes, all in sync, all updated like normal Python. No queues, no locks, no Manager, no pickle errors. No redesigning your code around what the serializer can handle.</p>
    <p><code>processing</code> makes parallel code regular code.</p>
    <hr>
    <h2>Python has a parallel processing problem.</h2>
    <p>Python uses a Global Interpreter Lock (GIL).</p>
    <p>What this means</p>
    <ul>
        <li>no memory leaks or corruption</li>
        <li>no garbage collection issues</li>
        <li>thread safe built-in types</li>
        <li>can work with C extensions</li>
    </ul>
    <p>Hey, that sounds pretty good!</p>
    <h3>Maybe it doesn&#x27;t sound that good</h3>
    <p>Python also has one major issue: no simultaneous bytecode execution.</p>
    <p>This means that threads don&#x27;t speed up CPU-bound work, and makes Python essentially single-threaded.</p>
    <p>16 cores running 16 threads will not speed up work.</p>
    <p>In a time where almost all major programs parallelize their code, Python is stuck in the past.</p>
    <h3>Or is it?</h3>
    <p>We can spawn multiple processes using Python&#x27;s <code>multiprocessing</code> module.</p>
    <ul>
        <li>each with their own GIL</li>
        <li>each with their own interpreter</li>
        <li>each with their own memory space</li>
    </ul>
    <p>True parallelism. Problem solved!</p>
    <p>Wrong. The problem is not solved yet.</p>
    <h3>1. <code>pickle</code> can&#x27;t serialize your code</h3>
    <p>You are trying to run something in parallel.</p>
    <pre><code class="language-python">def process_data(items):
    def transform(x):
        return x * 2

    for item in items:
        result = transform(item)
        results.append(result)
    return results

    
with Pool(4) as pool:
    return pool.map(transform, lists_of_items)</code></pre>
    <p>This looks like it should work. But it doesn&#x27;t, because you put a locally-defined function in the pool.</p>
    <p>There are a bunch of random things that <code>pickle</code> can&#x27;t handle, many of which are pretty common things you use when writing code.</p>
    <ul>
        <li>locally-defined functions</li>
        <li>lambdas</li>
        <li>closures</li>
        <li>dynamically created classes</li>
        <li>and most other complex code patterns that you would actually use when multiprocessing</li>
    </ul>
    <p>Figuring out what works and what doesn&#x27;t is a nightmare.</p>
    <h4>Just use <code>cloudpickle</code> or <code>dill</code>. They&#x27;re better than <code>pickle</code>.</h4>
    <p><code>cloudpickle</code> and <code>dill</code> are cool but they just lessen this problem, not solve it.</p>
    <ul>
        <li><code>cloudpickle</code> is fast, and has a little more coverage than <code>pickle</code>, but not enough</li>
        <li><code>dill</code> has a lot more coverage than <code>pickle</code>, but is very slow in exchange</li>
    </ul>
    <p>However: Python&#x27;s <code>multiprocessing</code> doesn&#x27;t use them by default.</p>
    <p>The standard library&#x27;s <code>multiprocessing.Pool</code> is hardcoded to use <code>pickle</code>. To use <code>cloudpickle</code> or <code>dill</code>, you have to:</p>
    <pre><code class="language-python"># option 1: monkey-patch the serializer (risky, affects entire process)
import multiprocessing
import cloudpickle

multiprocessing.reduction.ForkingPickler.dumps = cloudpickle.dumps
multiprocessing.reduction.ForkingPickler.loads = cloudpickle.loads

# let&#x27;s hope nothing else in your codebase depends on regular pickle</code></pre>
    <pre><code class="language-python"># option 2: use multiprocess (a fork of multiprocessing that uses dill)
# pip install multiprocess
import multiprocess as mp 

with mp.Pool(4) as pool:
    results = pool.map(my_function, items)

# dill is slow
# 2 libraries to keep track of</code></pre>
    <pre><code class="language-python"># option 3: use concurrent.futures with a custom executor
from concurrent.futures import ProcessPoolExecutor
import cloudpickle

# you need to write a custom executor class that overrides the serializer</code></pre>
    <p>None of these just work, but they all do waste your time.</p>
    <p>And even after all that, you still have limitations. Your code is now more complex, in exchange for... not as many serialization errors?</p>
    <h4>So what CAN you do?</h4>
    <p>You could learn every limitation of whatever you use, and tiptoe around objects or patterns that will fail.</p>
    <p>You could learn every limitation of whatever you use, and write custom code to handle unsupported objects haphazardly.</p>
    <p>Or, you could just use <code>processing</code>.</p>
    <h2><code>processing</code> uses <code>cucumber</code> instead of <code>pickle</code></h2>
    <p>By default, <code>processing</code> uses <code>cucumber</code>, <code>suitkaise</code>&#x27;s serialization engine, instead of <code>pickle</code>.</p>
    <h3>Problem actually solved</h3>
    <p><code>cucumber</code> handles everything.</p>
    <ul>
        <li>handles everything from ints to complex user created classes with live connections</li>
        <li>better than <code>pickle</code></li>
        <li>better than <code>cloudpickle</code></li>
        <li>better than <code>dill</code></li>
        <li>automatically used by <code>processing</code></li>
    </ul>
    <p><code>cucumber</code> actually solves the problem of things not being serializable. And the problem of actually being compatible with multiprocessing.</p>
    <p>(For more info, see the <code>cucumber</code> pages)</p>
    <h2>Python&#x27;s <code>multiprocessing</code> module also has problems</h2>
    <p>Python&#x27;s <code>multiprocessing</code> module also has problems.</p>
    <p>Outside of the serialization problem, a large problem still exists with <code>multiprocessing</code>.</p>
    <h3>2. Using <code>multiprocessing</code> is complicated and not intuitive</h3>
    <p><code>multiprocessing</code> is a powerful tool, but it is also a pain in the ass to actually use, especially for complex tasks. A lot of this is just due to the fact that you sort of have to actually manage everything yourself.</p>
    <p>Python gives us the bare minimum to parallelize code, but outside of that, everything is left to you.</p>
    <ul>
        <li>setup</li>
        <li>cleanup</li>
        <li>teardown</li>
        <li>sharing state</li>
        <li>error handling</li>
        <li>performance timing</li>
        <li>crash handling</li>
        <li>looping code</li>
        <li>more</li>
        <li>and your actual task you need to do</li>
    </ul>
    <p>This is a long list of things that you need in order to have solid code when parallelizing.</p>
    <h4>Making this situation worse</h4>
    <p>Notice what is passed into <code>multiprocessing</code> to run a single parallel process.</p>
    <pre><code class="language-python">import multiprocessing

def process_data(items):
    
    # process your data
    return data


# run the single process (not even in a Pool)
process = multiprocessing.Process(target=process_data, args=(items,))
process.start()</code></pre>
    <p>It&#x27;s a function.</p>
    <p>You have to add all of those things from that list into a single function.</p>
    <p>For the case above, passing in a simple function is fine, you just want to get compute and get the data faster in parallel.</p>
    <p>But the case above doesn&#x27;t scratch the surface of what you could do with parallel processing.</p>
    <p>Not only does having to pass a function make implementing and debugging very difficult, but it also goes against the entire point of object-oriented programming -- where you encapsulate your code into different class objects -- by forcing you to make one giant god function that does everything.</p>
    <h4>Making the situation better</h4>
    <p>What is something in programming that we can use to split up a giant god function into a more manageable set of pieces?</p>
    <p>Classes.</p>
    <p>Everyone knows how to work with classes. They are the fundamental building block of object-oriented programming.</p>
    <p>So why not pass a class into <code>multiprocessing</code> instead of a function?</p>
    <pre><code class="language-python">import multiprocessing

class ProcessData(multiprocessing.Process):
    def __init__(self, items, result_queue):
        self.items = items
        self.result_queue = result_queue  # need a Queue to communicate
        super().__init__()
    
    def run(self):
        # do work
        result = process_items(self.items)
        self.result_queue.put(result)  # send back via queue

queue = multiprocessing.Queue()
process = ProcessData(items, queue)
process.start()
process.join()
result = queue.get()  # retrieve from queue, not process.result</code></pre>
    <p>This is a step in the right direction, but it is by no means perfect.</p>
    <ul>
        <li>still sort of confusing in general</li>
        <li>you have to manually manage the queue</li>
        <li>this will still use base <code>pickle</code></li>
        <li>you have to manually call <code>super().__init__()</code> and implement <code>run()</code></li>
        <li>still no automatic retries, timeouts, timing, or error handling</li>
    </ul>
    <h2><code>Skprocess</code></h2>
    <p>A class is the overall solution that should&#x27;ve been used all along.</p>
    <p>But we still have no structure and no lifecycle.</p>
    <ul>
        <li>missing actual methods to split up code into smaller pieces</li>
        <li>missing good error handling</li>
        <li>hard to share state, must bring in a different object just for that</li>
        <li>overall, code is still missing a lot of the structure and automation that is expected</li>
    </ul>
    <p><code>Skprocess</code> is a class that goes above and beyond for you.</p>
    <ul>
        <li>automatically uses <code>cucumber</code></li>
        <li>supports <code>Share</code> (very important later)</li>
        <li>provides standard lifecycle methods to help you split up code into smaller pieces</li>
        <li>all result gathering is done using attributes and regular return statements</li>
        <li>clear error handling, even telling you what part of the code it failed on</li>
        <li>retries when the process crashes</li>
        <li>live resources can automatically reconnect</li>
        <li>automatic timing of every lifecycle method</li>
        <li>simple shared state, any object can be shared</li>
        <li>code loops for you</li>
        <li>no need to call <code>super().__init__()</code> when inheriting</li>
        <li>high level of control using a simple config</li>
        <li>fast and controlled bidirectional communication with <code>Pipe</code></li>
    </ul>
    <p>Let&#x27;s make a process that queries a database for user data based on given input from a parent process.</p>
    <p>Requirements:</p>
    <ul>
        <li>Receive query parameters from parent</li>
        <li>Connect to database and execute query</li>
        <li>Handle connection failures with retry</li>
        <li>Timeout if query takes too long</li>
        <li>Track timing statistics</li>
        <li>Return results to parent</li>
        <li>Clean up connection on exit</li>
    </ul>
    <p>Without <code>Skprocess</code> - <em>92 lines</em></p>
    <pre><code class="language-python"># comments and whitespace excluded from line count
import multiprocessing
import signal
import time
import psycopg2
from multiprocessing import Queue, Event, Value
from ctypes import c_double

class DatabaseWorker(multiprocessing.Process):
    def __init__(self, task_queue, result_queue, stats_lock, 
                 total_time, query_count, stop_event, db_config):
        super().__init__()
        self.task_queue = task_queue
        self.result_queue = result_queue
        self.stats_lock = stats_lock
        self.total_time = total_time
        self.query_count = query_count
        self.stop_event = stop_event
        self.db_config = db_config
        self.timeout = 30
        self.max_retries = 3
        self.conn = None
    
    def _connect(self):
        # manual retry logic with exponential backoff
        for attempt in range(self.max_retries):
            try:
                self.conn = psycopg2.connect(**self.db_config)
                return
            except psycopg2.OperationalError:
                if attempt == self.max_retries - 1:
                    raise
                time.sleep(2 ** attempt)
    
    def _timeout_handler(self, signum, frame):
        raise TimeoutError(&quot;Query timed out&quot;)
    
    def run(self):
        # manual connection setup
        self._connect()
        
        # manual signal handling for timeouts
        signal.signal(signal.SIGALRM, self._timeout_handler)
        
        try:
            while not self.stop_event.is_set():
                # check for incoming query (non-blocking)
                try:
                    query_params = self.task_queue.get(timeout=0.1)
                except:
                    # no query received
                    self.result_queue.put({&#x27;status&#x27;: &#x27;no query&#x27;, &#x27;data&#x27;: None})
                    continue
                
                # manual timing
                start = time.time()
                signal.alarm(self.timeout)
                
                try:
                    cursor = self.conn.cursor()
                    cursor.execute(query_params[&#x27;sql&#x27;], query_params.get(&#x27;params&#x27;))
                    results = cursor.fetchall()
                    cursor.close()
                    
                    signal.alarm(0)
                    elapsed = time.time() - start
                    
                    # manual stats tracking with locks
                    with self.stats_lock:
                        self.total_time.value += elapsed
                        self.query_count.value += 1
                    
                    # different status based on results
                    if not results:
                        self.result_queue.put({&#x27;status&#x27;: &#x27;error&#x27;, &#x27;data&#x27;: None})
                    else:
                        self.result_queue.put({&#x27;status&#x27;: &#x27;ok&#x27;, &#x27;data&#x27;: results})
                    
                except TimeoutError:
                    signal.alarm(0)
                    self.result_queue.put({&#x27;status&#x27;: &#x27;error&#x27;, &#x27;error&#x27;: &#x27;timeout&#x27;})
                except Exception as e:
                    signal.alarm(0)
                    self.result_queue.put({&#x27;status&#x27;: &#x27;error&#x27;, &#x27;error&#x27;: str(e)})
        finally:
            # manual cleanup
            if self.conn:
                self.conn.close()


# usage

# connect to the database (credentials stored separately)
db_config = {&#x27;host&#x27;: &#x27;localhost&#x27;, &#x27;database&#x27;: &#x27;mydb&#x27;, &#x27;password&#x27;: &#x27;secret&#x27;}

# create all the shared state machinery
manager = multiprocessing.Manager()
task_queue = Queue()
result_queue = Queue()
stats_lock = manager.Lock()
total_time = Value(c_double, 0.0)
query_count = Value(&#x27;i&#x27;, 0)
stop_event = Event()

# init and start the worker process
worker = DatabaseWorker(
    task_queue, result_queue, stats_lock, total_time, 
    query_count, stop_event, db_config,
    timeout=30, max_retries=3
)
worker.start()

# list of queries to run (counted as a single line)
queries = [
    {&#x27;sql&#x27;: &#x27;SELECT * FROM users WHERE id = %s&#x27;, &#x27;params&#x27;: (123,)},
    {&#x27;sql&#x27;: &#x27;SELECT * FROM users WHERE id = %s&#x27;, &#x27;params&#x27;: (456,)},
    {&#x27;sql&#x27;: &#x27;SELECT * FROM users WHERE id = %s&#x27;, &#x27;params&#x27;: (789,)},
    # ...
]

# create a list to store the results
results = []

# send each query to the worker
for query in queries:
    task_queue.put(query)
    result = result_queue.get(timeout=30)  # need manual timeout here too
    results.append(result)

# signal stop and wait for cleanup
stop_event.set()
worker.join()

# manual timing calculation
if query_count.value &gt; 0:
    avg_time = total_time.value / query_count.value
    print(f&quot;Avg query time: {avg_time:.3f}s&quot;)</code></pre>
    <p>There are a lot of problems here.</p>
    <ol>
        <li>6 import statements</li>
        <li>12 parameters in <code>__init__</code>, most of which are just trying to setup infrastructure</li>
        <li><code>super().__init__()</code> has to be called and is easy to forget</li>
        <li>manual retry logic</li>
        <li>manual performance timing</li>
        <li>multiple different timeouts need to be handled manually</li>
        <li>several queues to manage</li>
        <li>have to handle signals, which don&#x27;t even work on Windows</li>
        <li>awkward <code>.value</code> access for shared state</li>
        <li>have to use a separate event object for stopping</li>
        <li><code>Manager</code> for locks, something else that needs to be coordinated</li>
        <li>manual cleanup in <code>finally</code></li>
        <li>statistics done by hand</li>
        <li>passing database credentials around as a dict</li>
        <li>uses <code>pickle</code></li>
    </ol>
    <p>This is a simple example: you already have to do this much for this little.</p>
    <p>With <code>Skprocess</code> - <em>40 lines</em></p>
    <pre><code class="language-python"># comments and whitespace excluded from line count
from suitkaise.processing import Skprocess, autoreconnect
import psycopg2

@autoreconnect(**{&quot;psycopg2.Connection&quot;: {&quot;*&quot;: &quot;password&quot;}})
class DatabaseWorker(Skprocess):

    def __init__(self, db_connection)

        # this automatically reconnects
        self.db = db_connection

        # built in configuration
        # run indefinitely until stop() is called
        self.process_config.runs = None
        # NOTE: this is the default: here for clarity, not counted in line count

        # 3 lives (2 extra attempts after the first failure)
        self.process_config.lives = 3

        # 30 second timeout per query
        self.process_config.timeouts.run = 30.0
    
    def __prerun__(self):
        # receive query from parent (non-blocking check)
        msg = self.listen(timeout=0.1)
        self.query = msg if msg else None
    
    def __run__(self):
        if not self.query:
            return
        cursor = self.db.cursor()
        cursor.execute(self.query[&#x27;sql&#x27;], self.query.get(&#x27;params&#x27;))
        self.results = cursor.fetchall()
        cursor.close()
    
    def __postrun__(self):

        if self.query:
            if not self.results:
                self.tell({&#x27;status&#x27;: &#x27;error&#x27;, &#x27;data&#x27;: None})
            else:
                self.tell({&#x27;status&#x27;: &#x27;ok&#x27;, &#x27;data&#x27;: self.results})

        else:
            self.tell({&#x27;status&#x27;: &#x27;no query&#x27;, &#x27;data&#x27;: None})
    
    def __onfinish__(self):
        self.db.close()

# usage

# connect to the database
db = psycopg2.connect(host=&#x27;localhost&#x27;, database=&#x27;mydb&#x27;, password=&#x27;secret&#x27;)

# init and start the worker process
worker = DatabaseWorker(db)
worker.start()

# list of queries to run (counted as a single line)
queries = [
    {&#x27;sql&#x27;: &#x27;SELECT * FROM users WHERE id = %s&#x27;, &#x27;params&#x27;: (123,)},
    {&#x27;sql&#x27;: &#x27;SELECT * FROM users WHERE id = %s&#x27;, &#x27;params&#x27;: (456,)},
    {&#x27;sql&#x27;: &#x27;SELECT * FROM users WHERE id = %s&#x27;, &#x27;params&#x27;: (789,)},
    # ...
]

# create a list to store the results
results = []

# send each query to the worker
for query in queries:
    worker.tell(query)
    result = worker.listen(timeout=timeout)
    results.append(result)

# request stop (join) and wait for it to finish
worker.stop()
worker.wait()

# automatically timed
print(f&quot;Avg query time: {worker.__run__.timer.mean:.3f}s&quot;)</code></pre>
    <ul>
        <li>2x less code</li>
        <li>1 import line for all of your multiprocessing</li>
        <li>nothing is manual</li>
        <li>everything is organized</li>
        <li>uses <code>cucumber</code> for serialization</li>
        <li>automatically reconnects the db connection</li>
        <li>times, errors, and statistics are handled</li>
    </ul>
    <h2>Shared state in Python</h2>
    <p>Sharing state across process boundaries is one of the most functionally important parts of multiprocessing in Python.</p>
    <p>There are generally 6 patterns for doing this:</p>
    <ol>
        <li><code>Value</code> and <code>Array</code></li>
    </ol>
    <p>Shared memory, but just for primitive types.</p>
    <p>Pros: Fast, no serialization Cons: Only primitives, not even dicts or lists</p>
    <ol start="2">
        <li><code>multiprocessing.Manager</code></li>
    </ol>
    <p>Proxy objects that wrap Python types.</p>
    <p>Pros: Supports dict, list, and other Python types (uses <code>pickle</code>) Cons: Slow. Manager is a separate process, so not truly shared memory</p>
    <ol start="3">
        <li><code>multiprocessing.shared_memory</code></li>
    </ol>
    <p>Raw shared memory blocks. Only available in Python 3.8+.</p>
    <p>Pros: True shared memory, fast Cons: Manual buffer management, no serialization, have to handle syncing yourself</p>
    <ol start="4">
        <li>Queues (message passing)</li>
    </ol>
    <p>Isn&#x27;t exactly shared state, but functionally similar.</p>
    <p>Pros: Safe, decently fasts, works with any serializable object Cons: Not actually shared - each process has its own copy</p>
    <ol start="5">
        <li>Files/Databases</li>
    </ol>
    <p>Write to disk so that other processes can read.</p>
    <p>Pros: simple and persistent Cons: slow IO, race conditions, not real-time</p>
    <ol start="6">
        <li>External services</li>
    </ol>
    <p>Use an external process to hold state, like Redis or Memcached.</p>
    <p>Pros: atomic operations, pub/sub, works across machines Cons: External depedency, network overhead, more to manage</p>
    <p>The problem with all of these:</p>
    <ul>
        <li>none of these are easy or simple in practice</li>
        <li>you have to choose the right mechanism</li>
        <li>handle serialization, or be limited in what you can share</li>
        <li>sync manually</li>
        <li>lots of boilerplate</li>
    </ul>
    <h2><code>Share</code></h2>
    <p><code>Share</code> is the ultimate solution to shared state in Python.</p>
    <p>Pros:</p>
    <ul>
        <li>literally add any object to it and it will work the same exact way in shared memory</li>
        <li>as simple as it gets</li>
        <li>uses <code>cucumber</code> for serialization, so all objects work</li>
        <li>ensures that everything stays in sync</li>
        <li>works across any number of processes</li>
    </ul>
    <p>Cons:</p>
    <ul>
        <li>slowest overall option</li>
        <li>overhead</li>
        <li>cannot share <code>multiprocessing.*</code> objects (Python limitation)</li>
    </ul>
    <pre><code class="language-python">from suitkaise.processing import Skprocess, Share, Pool
import logging

share = Share()
share.counter = 0
share.log = logging.getLogger(&quot;ShareLog&quot;)



class ShareCounter(Skprocess):

    # pass the Share instance to the process
    def __init__(self, share: Share):
        self.share = share
        self.process_config.runs = 10

    def __run__(self):
        self.share.counter += 1
        self.share.log.info(f&quot;{self.share.counter}&quot;)



# just add share
process = MyProcess(share)

pool = Pool(workers=4)
pool.map(ShareCounter, [share] * 10)

print(share.counter) # 100 (10 total workers, 10 runs each)
print(share.log.messages) # [&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, ..., &#x27;100&#x27;] in order</code></pre>
    <p>In 20 lines, I made a counter that works in a parallel pool, that will loop exactly 10 times, logging its progress. Not a single result is needed, as everything just got added to shared memory, and that shared memory was 100% in sync.</p>
    <h3>100% worth the slow speed</h3>
    <p><code>Share</code> is about 3x slower than <code>multiprocessing.Manager</code>.</p>
    <p>But every object works exactly the same.</p>
    <p>To add them: assign them to attributes.</p>
    <p>To use them: access and update them as normal.</p>
    <h4>Why the 3x doesn&#x27;t matter in practice</h4>
    <p>The overhead is on the coordinator IPC layer, the per-operation cost of syncing state. For long-running parallel work (the kind where you actually need multiprocessing), that cost gets diluted as you perform longer running tasks.</p>
    <p>If your process runs for 30 seconds and does 1,000 share operations, the overhead is a few extra milliseconds total. Meanwhile, the alternative is hours of your time debugging <code>Manager</code> + <code>pickle</code> errors + race conditions + manual sync logic.</p>
    <p><code>Share</code> trades microseconds of IPC overhead for the ability to turn your brain off and never write shared state boilerplate again. You create it, assign to it, and read from it -- exactly like you learned in your first programming class.</p>
    <p>That&#x27;s a tradeoff worth making.</p>
    <h2><code>processing</code> still has 2 more options for sharing state</h2>
    <p><code>processing</code> has 2 high speed options for sharing state.</p>
    <h3>1. <code>Skprocess.tell()</code> and <code>Skprocess.listen()</code></h3>
    <p>The 2 queue-like methods that are a part of <code>Skprocess</code> (and all inheriting classes).</p>
    <p>These are automatically 2 way, and use <code>cucumber</code>.</p>
    <pre><code class="language-python">from suitkaise.processing import Skprocess

class MyProcess(Skprocess):
    def __prerun__(self):

        self.command = self.listen(timeout=1.0)

    def __run__(self):

        if self.command == &quot;stop&quot;:
            self.stop()

        elif self.command == &quot;print&quot;:
            print(&quot;hello&quot;)

        else:
            raise ValueError(f&quot;Unknown command: {self.command}&quot;)

    def __postrun__(self):
        self.command = None
        self.tell(&quot;command received&quot;)
        


p = MyProcess()
p.start()
for i in range(10):
    p.tell(&quot;print&quot;)
    result = p.listen(timeout=1.0)
    if result != &quot;command received&quot;:
        break

p.tell(&quot;stop&quot;)
p.wait()</code></pre>
    <h3>2. <code>Pipe</code></h3>
    <p>The fastest, most direct way to communicate between processes.</p>
    <pre><code class="language-python">from suitkaise.processing import Pipe, Skprocess

anchor, point = Pipe.pair()

class MyProcess(Skprocess):

    def __init__(self, pipe_point: Pipe.Point):
        self.pipe = pipe_point
        self.process_config.runs = 1

    def __run__(self):
        self.pipe.send(&quot;hello&quot;)
        result = self.pipe.recv()
        print(result)

process = MyProcess(point)
process.start()

anchor.send(&quot;hello&quot;)

result = anchor.recv()
print(result)

process.wait()</code></pre>
    <p>One way pipe:</p>
    <pre><code class="language-python">from suitkaise.processing import Pipe, Skprocess

# one way pipe: only anchor can send data, point can only receive
anchor, point = Pipe.pair(one_way=True)

class MyProcess(Skprocess):
    def __init__(self, pipe_point: Pipe.Point):
        self.pipe = pipe_point
        self.process_config.runs = 1

    def __prerun__(self):
        self.data_to_process = self.pipe.recv()

    def __run__(self):
        self.process_data(self.data_to_process)

    def __postrun__(self):
        self.data_to_process = None</code></pre>
    <h3>So, which one should you use?</h3>
    <p>Most of the time, you should just use <code>Share</code>.</p>
    <p>If you want simpler, faster, 2-way communication without setup, use <code>tell()</code> and <code>listen()</code>.</p>
    <p>But if you still need speed, or want more manual control, use <code>Pipe</code>.</p>
    <h2>Putting it all together</h2>
    <p>Throughout this page, you might have seen something called <code>Pool</code>.</p>
    <p><code>Pool</code> is an upgraded wrapper around <code>multiprocessing.Pool</code> used for parallel batch processing.</p>
    <p>What this enables:</p>
    <ul>
        <li>process pools support <code>Share</code></li>
        <li>process pools using <code>cucumber</code> for serialization</li>
        <li>process pools using <code>Skprocess</code> class objects</li>
        <li>process pools get access to <code>sk</code> modifiers</li>
    </ul>
    <p>So, already, <code>Pool</code> is vastly more powerful than <code>multiprocessing.Pool</code>. especially because you can use <code>Share</code>.</p>
    <h3><code>Pool</code> is better, but still familiar to users</h3>
    <p>It has the 4 main map methods, with clearer names.</p>
    <p><code>map</code>: returns a list, ordered by input. Each item gets added to the list in the order it was added to the pool.</p>
    <pre><code class="language-python">list_in_order = Pool.map(fn_or_skprocess, items)</code></pre>
    <p><code>unordered_map</code>: returns a list, unordered. Whatever finishes first, gets added to the list first.</p>
    <pre><code class="language-python">unordered_list = Pool.unordered_map(fn_or_skprocess, items)</code></pre>
    <p><code>imap</code>: returns an iterator, ordered by input. Each item gets added to the iterator in the order it was added to the pool.</p>
    <pre><code class="language-python">for item in Pool.imap(fn_or_skprocess, items):
    print(item)</code></pre>
    <p><code>unordered_imap</code>: returns an iterator, unordered. Whatever finishes first, gets added to the iterator first.</p>
    <pre><code class="language-python">for item in Pool.unordered_imap(fn_or_skprocess, items):
    print(item)</code></pre>
    <p>Since you can use <code>Skprocess</code> objects that can <code>stop()</code> themselves (or set a number of runs), you can theoretically keep running the pool and let the processes run until they are done. This opens up a lot of possibilities for complex parallel processing tasks.</p>
    <h3>Modifiers are what make it reach the next level</h3>
    <p><code>sk</code> modifiers are from another <code>suitkaise</code> module, and are available on most <code>suitkaise</code> functions and methods, including <code>Pool</code>.</p>
    <ul>
        <li>timeouts</li>
        <li>native async support</li>
        <li>background execution with <code>Future</code>s</li>
    </ul>
    <p>And, <code>Pool</code> itself has a special modifier, <code>star()</code>, that allows you to unpack tuples into function arguments.</p>
    <pre><code class="language-python">from suitkaise.processing import Pool
import asyncio

# get a coroutine for map with a timeout
coro = Pool.map.timeout(20.0).asynced()
results = await coro(fn_or_skprocess, items)

# or, run in the background, get a Future
# and unpack tuples across function arguments (instead of adding the whole tuple as a single argument)
future = Pool.star().map.background()(fn_or_skprocess, items)</code></pre>
    <p><code>asynced()</code> and <code>background()</code> do not work with each other (they do the same thing in different ways), but other than that, everything else is combinable.</p>
    <p>These modifiers work with all map methods.</p>
    <p>For more info on how to use these modifiers, see the <code>sk</code> pages or look at the <code>processing</code> examples.</p>
    <h2>Works with the rest of <code>suitkaise</code></h2>
    <p><code>processing</code> doesn&#x27;t exist in a vacuum. It&#x27;s designed to work with the rest of the <code>suitkaise</code> ecosystem.</p>
    <ul>
        <li><code>cucumber</code> handles all serialization automatically. You never think about pickle errors.</li>
        <li><code>timing</code> provides <code>Sktimer</code> objects that work natively inside <code>Share</code> -- aggregate timing statistics across processes without any extra code.</li>
        <li><code>sk</code> generates <code>_shared_meta</code> for your classes, which tells <code>Share</code> exactly which attributes each method reads and writes. This is what makes <code>Share</code> efficient.</li>
        <li><code>circuits</code> provides circuit breakers that work inside <code>Share</code> -- one process trips the circuit, every other process sees it immediately. Cross-process fault tolerance with zero setup.</li>
        <li><code>paths</code> gives you <code>Skpath</code> objects that serialize cleanly through <code>cucumber</code> and work the same on every machine.</li>
    </ul>
    <p>Each module is useful on its own, but they were designed together. When you use <code>processing</code>, you get the full benefit of that integration.</p>
</section>
