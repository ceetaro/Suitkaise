<div class="module-bar" data-module="timing">
    <button class="module-bar-title">suitkaise.timing</button>
    <nav class="module-bar-nav">
        <a href="#timing-why" class="module-bar-link" data-page="timing-why">why</a>
        <a href="#timing-quick-start" class="module-bar-link" data-page="timing-quick-start">quick start</a>
        <a href="#timing" class="module-bar-link" data-page="timing">how to use</a>
        <a href="#timing-how-it-works" class="module-bar-link" data-page="timing-how-it-works">how it works</a>
        <a href="#timing-examples" class="module-bar-link active" data-page="timing-examples">examples</a>
        <a href="#timing-videos" class="module-bar-link" data-page="timing-videos">videos</a>
        <a href="#timing-tests" class="module-bar-link" data-page="timing-tests">tests</a>
        <a href="#timing-learn" class="module-bar-link" data-page="timing-learn">learn</a>
    </nav>
</div>
<section class="module-page">
    <h1><code>timing</code> Examples</h1>
    <details>
        <summary>Basic Examples</summary>
        <div class="dropdown-content">
    <h3>Simple Timing</h3>
    <pre><code class="language-python">from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# Basic start/stop timing
# 
# The simplest way to time something: start, do work, stop.
# The elapsed time is returned by stop() and also stored in the timer.
# ──────────────────────────────────────────────────────────────────────────────

# create a new timer instance
# - no arguments needed for basic usage
timer = timing.Sktimer()

# start the timer
# - records current high-resolution timestamp
# - uses perf_counter() internally for accurate measurements
timer.start()

# do some work
# - this is what we&#x27;re measuring
# - could be any code: function calls, loops, I/O operations
for i in range(1000000):
    _ = i * i

# stop the timer and get elapsed time
# - returns elapsed time in seconds as a float
# - also stores the measurement in timer.times for statistics
elapsed = timer.stop()

# print the result
# - :.3f formats to 3 decimal places (millisecond precision)
print(f&quot;Elapsed: {elapsed:.3f}s&quot;)

# you can also access the last measurement via property
# - most_recent is an alias for the last recorded time
# - result is also an alias for most_recent
print(f&quot;Same value: {timer.most_recent:.3f}s&quot;)</code></pre>
    <h3>Using elapsed()</h3>
    <pre><code class="language-python">from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# Using elapsed() for simple time differences
#
# elapsed() calculates the difference between two timestamps.
# If you only provide one timestamp, it uses current time as the second.
# Order doesn&#x27;t matter - always returns positive value.
# ──────────────────────────────────────────────────────────────────────────────

# get the current Unix timestamp
# - equivalent to time.time()
# - returns seconds since epoch as a float
start = timing.time()

# do real work
import hashlib
payload = b&quot;elapsed_example&quot;
for _ in range(20000):
    payload = hashlib.sha256(payload).digest()

# calculate elapsed time with one argument
# - uses current time as the second timestamp
# - same as: timing.elapsed(start, timing.time())
elapsed = timing.elapsed(start)
print(f&quot;Elapsed: {elapsed:.3f}s&quot;)  # ~0.500s

# calculate elapsed time with two arguments
# - explicitly provide both timestamps
end = timing.time()
elapsed = timing.elapsed(start, end)
print(f&quot;Elapsed: {elapsed:.3f}s&quot;)

# order doesn&#x27;t matter - always returns positive value
# - internally uses abs() so you can&#x27;t get negative results
# - useful when you&#x27;re not sure which timestamp is earlier
elapsed_reversed = timing.elapsed(end, start)
print(f&quot;Reversed: {elapsed_reversed:.3f}s&quot;)  # same value</code></pre>
    <h3>Multiple Measurements</h3>
    <pre><code class="language-python">from suitkaise import timing
import hashlib

# ──────────────────────────────────────────────────────────────────────────────
# Collecting multiple measurements for statistics
#
# Run the same operation multiple times and collect timing data.
# Sktimer accumulates all measurements and provides statistical analysis.
# ──────────────────────────────────────────────────────────────────────────────

timer = timing.Sktimer()

# run 100 iterations of the same operation
# - each iteration is timed independently
# - all times are stored in timer.times list
for i in range(100):
    timer.start()
    
    # variable work based on input size
    payload = b&quot;x&quot; * (2000 + (i % 5) * 500)
    hashlib.sha256(payload).hexdigest()
    
    timer.stop()

# access statistics
# - num_times: how many measurements we collected
# - mean: average of all measurements
# - median: middle value when sorted (less affected by outliers)
# - stdev: standard deviation (measure of variance)
# - min/max: fastest and slowest measurements
print(f&quot;Measurements: {timer.num_times}&quot;)
print(f&quot;Mean: {timer.mean:.3f}s&quot;)
print(f&quot;Median: {timer.median:.3f}s&quot;)
print(f&quot;Std Dev: {timer.stdev:.3f}s&quot;)
print(f&quot;Min: {timer.min:.3f}s&quot;)
print(f&quot;Max: {timer.max:.3f}s&quot;)

# calculate percentiles
# - percentile(95) means 95% of measurements are at or below this value
# - useful for understanding &quot;worst case&quot; performance
p95 = timer.percentile(95)
p99 = timer.percentile(99)
print(f&quot;95th percentile: {p95:.3f}s&quot;)
print(f&quot;99th percentile: {p99:.3f}s&quot;)</code></pre>
    <h3>Using lap()</h3>
    <pre><code class="language-python">from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# Using lap() for continuous measurements
#
# lap() records the current measurement and immediately starts the next one.
# It&#x27;s like calling stop() + start() in one operation.
# Useful for timing iterations without the overhead of separate stop/start.
# ──────────────────────────────────────────────────────────────────────────────

timer = timing.Sktimer()

# start timing
timer.start()

# process multiple items, recording time for each
items = [&quot;item1&quot;, &quot;item2&quot;, &quot;item3&quot;, &quot;item4&quot;, &quot;item5&quot;]

for item in items:
    # real work per item
    import hashlib
    payload = (item * 1000).encode()
    hashlib.sha256(payload).hexdigest()
    
    # record lap time and continue
    # - records time since last lap() or start()
    # - immediately begins new measurement
    # - returns the elapsed time for this lap
    lap_time = timer.lap()
    print(f&quot;Processed {item}: {lap_time:.3f}s&quot;)

# after the loop, there&#x27;s still an active measurement running
# - we need to either stop() or discard() it
# - discard() stops timing without recording (since we already recorded with lap())
timer.discard()

# we have 5 measurements (one per lap)
print(f&quot;\nTotal measurements: {timer.num_times}&quot;)
print(f&quot;Total time: {timer.total_time:.3f}s&quot;)
print(f&quot;Average per item: {timer.mean:.3f}s&quot;)</code></pre>
    <h3>Pause and Resume</h3>
    <pre><code class="language-python">from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# Pausing timing during user interaction
#
# Sometimes you want to exclude certain time from measurements.
# pause()/resume() let you temporarily stop the clock without ending the session.
# Useful for excluding user input, network waits, or other external delays.
# ──────────────────────────────────────────────────────────────────────────────

timer = timing.Sktimer()

timer.start()

# phase 1: initial processing (timed)
print(&quot;Phase 1: Processing data...&quot;)
import hashlib
data = b&quot;phase1&quot;
for _ in range(20000):
    data = hashlib.sha256(data).digest()

# pause timing during user interaction
# - time spent paused is tracked separately
# - will be excluded from the final elapsed time
timer.pause()

# do work while paused (not timed)
# - in real code: user_input = input(&quot;Continue? &quot;)
print(&quot;Waiting for user input (not timed)...&quot;)
data = b&quot;paused_work&quot;
for _ in range(30000):
    data = hashlib.sha256(data).digest()

# resume timing
# - clock starts again from where it paused
timer.resume()

# phase 2: more processing (timed)
print(&quot;Phase 2: More processing...&quot;)
data = b&quot;phase2&quot;
for _ in range(20000):
    data = hashlib.sha256(data).digest()

# stop and get elapsed time
elapsed = timer.stop()

# elapsed should be ~0.4s, not ~1.4s
# - the 1.0s pause is excluded
print(f&quot;\nActive work time: {elapsed:.3f}s&quot;)
print(f&quot;Time spent paused: {timer.total_time_paused:.3f}s&quot;)</code></pre>
        </div>
    </details>
    <details>
        <summary>Context Manager Examples</summary>
        <div class="dropdown-content">
    <h3>Basic TimeThis</h3>
    <pre><code class="language-python">from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# TimeThis context manager for clean timing syntax
#
# TimeThis wraps a code block with automatic start/stop.
# The &#x27;as timer&#x27; gives you access to the timer inside and after the block.
# Perfect for one-off measurements without explicit start/stop calls.
# ──────────────────────────────────────────────────────────────────────────────

# time a code block using context manager
# - automatically calls start() on entry
# - automatically calls stop() on exit
# - creates a new Sktimer if none provided
with timing.TimeThis() as timer:
    # all code in this block is timed
    total = 0
    for i in range(1000000):
        total += i

# after the block, timer has the measurement
print(f&quot;Loop took: {timer.most_recent:.3f}s&quot;)

# compare two operations
# ─────────────────────────────────────────────────────────────────────────────

# time operation A
with timing.TimeThis() as timer_a:
    result_a = sum(range(1000000))

# time operation B
with timing.TimeThis() as timer_b:
    result_b = 0
    for i in range(1000000):
        result_b += i

print(f&quot;\nBuilt-in sum(): {timer_a.most_recent:.6f}s&quot;)
print(f&quot;Manual loop:   {timer_b.most_recent:.6f}s&quot;)
print(f&quot;Ratio: {timer_b.most_recent / timer_a.most_recent:.1f}x slower&quot;)</code></pre>
    <h3>Shared Timer with TimeThis</h3>
    <pre><code class="language-python">from suitkaise import timing
from pathlib import Path
import json
import hashlib

# ──────────────────────────────────────────────────────────────────────────────
# Accumulating statistics across multiple TimeThis blocks
#
# Pass a pre-created Sktimer to TimeThis to collect multiple measurements.
# Each context manager block adds one measurement to the shared timer.
# Great for timing the same operation in different parts of your code.
# ──────────────────────────────────────────────────────────────────────────────

# create a shared timer for all API-like file reads
api_timer = timing.Sktimer()

# seed local data files
data_dir = Path(&quot;data/api&quot;)
data_dir.mkdir(parents=True, exist_ok=True)
for user_id in range(5):
    (data_dir / f&quot;user_{user_id}.json&quot;).write_text(
        json.dumps({&quot;id&quot;: user_id, &quot;name&quot;: f&quot;User {user_id}&quot;})
    )
    (data_dir / f&quot;posts_{user_id}.json&quot;).write_text(
        json.dumps([{&quot;id&quot;: i, &quot;title&quot;: f&quot;Post {i}&quot;} for i in range(3)])
    )

def fetch_user(user_id):
    &quot;&quot;&quot;Fetch a user from disk.&quot;&quot;&quot;
    # pass the shared timer to TimeThis
    # - each call adds one measurement to api_timer
    with timing.TimeThis(api_timer):
        text = (data_dir / f&quot;user_{user_id}.json&quot;).read_text()
        digest = hashlib.sha256(text.encode()).hexdigest()
        return {**json.loads(text), &quot;digest&quot;: digest[:8]}

def fetch_posts(user_id):
    &quot;&quot;&quot;Fetch posts for a user from disk.&quot;&quot;&quot;
    with timing.TimeThis(api_timer):
        text = (data_dir / f&quot;posts_{user_id}.json&quot;).read_text()
        posts = json.loads(text)
        for post in posts:
            post[&quot;hash&quot;] = hashlib.sha256(post[&quot;title&quot;].encode()).hexdigest()[:8]
        return posts

# make several API calls
# - each call is timed and added to api_timer
for user_id in range(5):
    user = fetch_user(user_id)
    posts = fetch_posts(user_id)

# analyze combined API performance
# - 5 users × 2 calls each = 10 measurements
print(f&quot;Total API calls: {api_timer.num_times}&quot;)
print(f&quot;Total API time: {api_timer.total_time:.3f}s&quot;)
print(f&quot;Average call: {api_timer.mean:.3f}s&quot;)
print(f&quot;Slowest call: {api_timer.max:.3f}s&quot;)
print(f&quot;95th percentile: {api_timer.percentile(95):.3f}s&quot;)</code></pre>
    <h3>TimeThis with Threshold</h3>
    <pre><code class="language-python">from suitkaise import timing
import hashlib

# ──────────────────────────────────────────────────────────────────────────────
# Filtering out fast operations with threshold
#
# The threshold parameter only records times above a minimum value.
# Useful when you only care about &quot;slow&quot; operations for analysis.
# Fast operations are silently discarded.
# ──────────────────────────────────────────────────────────────────────────────

slow_timer = timing.Sktimer()

def process_item(item):
    &quot;&quot;&quot;Process an item, sometimes slow.&quot;&quot;&quot;
    # only record times &gt;= 0.1 seconds
    # - fast operations won&#x27;t be recorded
    # - helps focus analysis on problematic cases
    with timing.TimeThis(slow_timer, threshold=0.1):
        # variable processing time based on item size
        data = f&quot;item_{item}&quot;.encode()
        iterations = 40000 if item % 5 == 0 else 4000
        for _ in range(iterations):
            data = hashlib.sha256(data).digest()

# process 50 items
for i in range(50):
    process_item(i)

# only slow operations were recorded
# - expected: ~10 measurements (20% of 50)
print(f&quot;Slow operations detected: {slow_timer.num_times}&quot;)
if slow_timer.num_times &gt; 0:
    print(f&quot;Average slow time: {slow_timer.mean:.3f}s&quot;)</code></pre>
        </div>
    </details>
    <details>
        <summary>Decorator Examples</summary>
        <div class="dropdown-content">
    <h3>Basic @timethis</h3>
    <pre><code class="language-python">from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# Timing functions with @timethis decorator
#
# @timethis() automatically times every call to the decorated function.
# The timer is attached to the function as .timer attribute.
# Call the function multiple times to build statistics.
# ──────────────────────────────────────────────────────────────────────────────

@timing.timethis()
def fibonacci(n):
    &quot;&quot;&quot;Calculate nth Fibonacci number (inefficient recursive version).&quot;&quot;&quot;
    if n &lt;= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)

# first call
result = fibonacci(20)
print(f&quot;fib(20) = {result}&quot;)
print(f&quot;Time: {fibonacci.timer.most_recent:.3f}s&quot;)

# call multiple times to build statistics
# - each call adds a measurement to fibonacci.timer
for n in [15, 18, 20, 22, 25]:
    result = fibonacci(n)
    print(f&quot;fib({n}) = {result}, time: {fibonacci.timer.most_recent:.3f}s&quot;)

# view statistics
print(f&quot;\nTotal calls: {fibonacci.timer.num_times}&quot;)
print(f&quot;Mean time: {fibonacci.timer.mean:.3f}s&quot;)
print(f&quot;Min time: {fibonacci.timer.min:.3f}s&quot;)
print(f&quot;Max time: {fibonacci.timer.max:.3f}s&quot;)</code></pre>
    <h3>Shared Timer Across Functions</h3>
    <pre><code class="language-python">from suitkaise import timing
import hashlib

# ──────────────────────────────────────────────────────────────────────────────
# Single timer tracking multiple functions
#
# Pass an explicit Sktimer to @timethis() to share across functions.
# All decorated functions contribute to the same statistics.
# Useful for measuring total time spent on a category of operations.
# ──────────────────────────────────────────────────────────────────────────────

# create a shared timer for all math operations
math_timer = timing.Sktimer()

@timing.timethis(math_timer)
def add(a, b):
    hashlib.sha256(f&quot;{a}+{b}&quot;.encode()).digest()
    return a + b

@timing.timethis(math_timer)
def multiply(a, b):
    hashlib.sha256(f&quot;{a}*{b}&quot;.encode()).digest()
    return a * b

@timing.timethis(math_timer)
def divide(a, b):
    hashlib.sha256(f&quot;{a}/{b}&quot;.encode()).digest()
    return a / b if b != 0 else 0

# perform many operations
# - all times go into math_timer
for i in range(1, 101):
    a, b = i, i + 1
    add(a, b)
    multiply(a, b)
    divide(a, b)

# combined statistics across all math functions
# - 100 iterations × 3 functions = 300 measurements
print(f&quot;Total math operations: {math_timer.num_times}&quot;)
print(f&quot;Total math time: {math_timer.total_time:.3f}s&quot;)
print(f&quot;Average operation: {math_timer.mean:.6f}s&quot;)</code></pre>
    <h3>Stacked Decorators</h3>
    <pre><code class="language-python">from suitkaise import timing
from pathlib import Path
import json

# ──────────────────────────────────────────────────────────────────────────────
# Both shared and per-function timing
#
# Stack multiple @timethis() decorators to track at different granularities.
# One timer for combined stats, another for per-function stats.
# Useful for detailed performance analysis.
# ──────────────────────────────────────────────────────────────────────────────

# shared timer for all database operations
db_timer = timing.Sktimer()

@timing.timethis()           # per-function timer (auto-attached)
@timing.timethis(db_timer)   # shared timer
def db_read(key):
    &quot;&quot;&quot;Read from a JSON file as a tiny local store.&quot;&quot;&quot;
    data = json.loads(db_path.read_text())
    return data.get(key)

@timing.timethis()           # per-function timer (auto-attached)
@timing.timethis(db_timer)   # shared timer
def db_write(key, value):
    &quot;&quot;&quot;Write to a JSON file as a tiny local store.&quot;&quot;&quot;
    data = json.loads(db_path.read_text())
    data[key] = value
    db_path.write_text(json.dumps(data))
    return True

db_path = Path(&quot;data/db.json&quot;)
db_path.parent.mkdir(parents=True, exist_ok=True)
db_path.write_text(json.dumps({}))

# perform operations
for i in range(20):
    db_read(f&quot;key_{i}&quot;)
    db_write(f&quot;key_{i}&quot;, f&quot;value_{i}&quot;)

# combined database statistics
print(&quot;=== Combined DB Stats ===&quot;)
print(f&quot;Total operations: {db_timer.num_times}&quot;)  # 40
print(f&quot;Total time: {db_timer.total_time:.3f}s&quot;)
print(f&quot;Mean: {db_timer.mean:.3f}s&quot;)

# per-function statistics
print(&quot;\n=== Read Stats ===&quot;)
print(f&quot;Read operations: {db_read.timer.num_times}&quot;)  # 20
print(f&quot;Read mean: {db_read.timer.mean:.3f}s&quot;)

print(&quot;\n=== Write Stats ===&quot;)
print(f&quot;Write operations: {db_write.timer.num_times}&quot;)  # 20
print(f&quot;Write mean: {db_write.timer.mean:.3f}s&quot;)</code></pre>
    <h3>Rolling Window</h3>
    <pre><code class="language-python">from suitkaise import timing
import hashlib

# ──────────────────────────────────────────────────────────────────────────────
# Rolling window for recent measurements only
#
# max_times limits how many measurements are kept.
# Older measurements are automatically discarded.
# Useful for long-running processes where you only care about recent performance.
# ──────────────────────────────────────────────────────────────────────────────

# only keep last 10 measurements
# - older measurements are discarded as new ones arrive
# - memory stays bounded regardless of how many calls
@timing.timethis(max_times=10)
def process_request():
    &quot;&quot;&quot;Process a request by hashing its payload.&quot;&quot;&quot;
    payload = b&quot;request_payload&quot; * 500
    hashlib.sha256(payload).hexdigest()

# process 100 requests
for i in range(100):
    process_request()
    
    # check stats periodically
    if (i + 1) % 25 == 0:
        # num_times is capped at 10 (our max_times)
        print(f&quot;After {i + 1} requests: &quot;
              f&quot;{process_request.timer.num_times} measurements, &quot;
              f&quot;mean: {process_request.timer.mean:.3f}s&quot;)

# final stats are based on only the last 10 requests
print(f&quot;\nFinal stats (last 10 only):&quot;)
print(f&quot;Mean: {process_request.timer.mean:.3f}s&quot;)
print(f&quot;Min: {process_request.timer.min:.3f}s&quot;)
print(f&quot;Max: {process_request.timer.max:.3f}s&quot;)</code></pre>
        </div>
    </details>
    <details>
        <summary>Advanced Examples</summary>
        <div class="dropdown-content">
    <h3>Concurrent Timing</h3>
    <pre><code class="language-python">from suitkaise import timing
import threading
import hashlib

# ──────────────────────────────────────────────────────────────────────────────
# Thread-safe timing across multiple threads
#
# Sktimer is fully thread-safe using per-thread sessions.
# Multiple threads can time operations concurrently.
# All measurements aggregate into a single statistics pool.
# ──────────────────────────────────────────────────────────────────────────────

timer = timing.Sktimer()

def worker(worker_id, iterations):
    &quot;&quot;&quot;Worker function that times its operations.&quot;&quot;&quot;
    for i in range(iterations):
        timer.start()
        
        # real work with deterministic variation
        payload = f&quot;worker_{worker_id}_{i}&quot;.encode()
        iterations = 2000 + (i % 5) * 500
        for _ in range(iterations):
            payload = hashlib.sha256(payload).digest()
        
        timer.stop()
    
    print(f&quot;Worker {worker_id} completed {iterations} iterations&quot;)

# create and start multiple threads
# - each thread times its own work independently
# - all times go into the same timer
threads = []
for i in range(4):
    t = threading.Thread(target=worker, args=(i, 25))
    threads.append(t)
    t.start()

# wait for all threads to complete
for t in threads:
    t.join()

# combined statistics from all threads
# - 4 workers × 25 iterations = 100 measurements
print(f&quot;\n=== Combined Stats (all threads) ===&quot;)
print(f&quot;Total measurements: {timer.num_times}&quot;)
print(f&quot;Total time: {timer.total_time:.3f}s&quot;)
print(f&quot;Mean: {timer.mean:.3f}s&quot;)
print(f&quot;Std Dev: {timer.stdev:.3f}s&quot;)</code></pre>
    <h3>Benchmarking Multiple Implementations</h3>
    <pre><code class="language-python">from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# Comparing performance of different implementations
#
# Create separate timers for each implementation.
# Run multiple iterations to get statistically meaningful results.
# Compare using mean, std dev, and percentiles.
# ──────────────────────────────────────────────────────────────────────────────

def benchmark(name, func, iterations=100):
    &quot;&quot;&quot;Run a function multiple times and return timing statistics.&quot;&quot;&quot;
    timer = timing.Sktimer()
    
    for _ in range(iterations):
        timer.start()
        func()
        timer.stop()
    
    return {
        &#x27;name&#x27;: name,
        &#x27;mean&#x27;: timer.mean,
        &#x27;stdev&#x27;: timer.stdev,
        &#x27;p50&#x27;: timer.percentile(50),
        &#x27;p95&#x27;: timer.percentile(95),
        &#x27;p99&#x27;: timer.percentile(99),
    }

# implementations to compare
def list_append():
    result = []
    for i in range(10000):
        result.append(i)
    return result

def list_comprehension():
    return [i for i in range(10000)]

def list_constructor():
    return list(range(10000))

# run benchmarks
results = [
    benchmark(&quot;list.append()&quot;, list_append),
    benchmark(&quot;list comprehension&quot;, list_comprehension),
    benchmark(&quot;list(range())&quot;, list_constructor),
]

# print comparison table
print(f&quot;{&#x27;Method&#x27;:&lt;20} {&#x27;Mean&#x27;:&gt;10} {&#x27;StdDev&#x27;:&gt;10} {&#x27;P95&#x27;:&gt;10} {&#x27;P99&#x27;:&gt;10}&quot;)
print(&quot;-&quot; * 62)

for r in sorted(results, key=lambda x: x[&#x27;mean&#x27;]):
    print(f&quot;{r[&#x27;name&#x27;]:&lt;20} &quot;
          f&quot;{r[&#x27;mean&#x27;]*1000:&gt;9.3f}ms &quot;
          f&quot;{r[&#x27;stdev&#x27;]*1000:&gt;9.3f}ms &quot;
          f&quot;{r[&#x27;p95&#x27;]*1000:&gt;9.3f}ms &quot;
          f&quot;{r[&#x27;p99&#x27;]*1000:&gt;9.3f}ms&quot;)</code></pre>
    <h3>Discard Failed Operations</h3>
    <pre><code class="language-python">from suitkaise import timing
import hashlib

# ──────────────────────────────────────────────────────────────────────────────
# Only recording successful operations
#
# Use discard() to stop timing without recording when an operation fails.
# Keeps statistics clean and meaningful (only successful times).
# The discarded time is still returned for logging if needed.
# ──────────────────────────────────────────────────────────────────────────────

timer = timing.Sktimer()
success_count = 0
failure_count = 0

def unreliable_operation(item_id: int):
    &quot;&quot;&quot;Operation that sometimes fails based on content.&quot;&quot;&quot;
    payload = f&quot;item_{item_id}&quot;.encode()
    digest = hashlib.sha256(payload).digest()
    
    # deterministic failure for some inputs
    if digest[0] % 3 == 0:
        raise RuntimeError(&quot;Operation failed&quot;)
    
    return digest[:8].hex()

# run many operations
for i in range(100):
    timer.start()
    
    try:
        result = unreliable_operation(i)
        # success - record the timing
        timer.stop()
        success_count += 1
        
    except RuntimeError:
        # failure - discard timing (don&#x27;t pollute statistics)
        # - returns elapsed time in case we want to log it
        discarded_time = timer.discard()
        failure_count += 1

# statistics only reflect successful operations
print(f&quot;Successful operations: {success_count}&quot;)
print(f&quot;Failed operations: {failure_count}&quot;)
print(f&quot;Recorded measurements: {timer.num_times}&quot;)  # equals success_count
print(f&quot;Mean success time: {timer.mean:.3f}s&quot;)</code></pre>
    <h3>Async Timing</h3>
    <pre><code class="language-python">import asyncio
from suitkaise import timing

# ──────────────────────────────────────────────────────────────────────────────
# Timing async operations
#
# The timing API works the same in async context.
# ──────────────────────────────────────────────────────────────────────────────

async def fetch_data(item_id):
    &quot;&quot;&quot;Async file read with real I/O.&quot;&quot;&quot;
    from pathlib import Path
    path = Path(f&quot;data/async_{item_id}.txt&quot;)
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(&quot;async data\n&quot; * 1000)
    return await asyncio.to_thread(path.read_text)

async def main():
    timer = timing.Sktimer()
    
    # time multiple async operations
    for i in range(5):
        timer.start()
        data = await fetch_data(i)
        elapsed = timer.stop()
        print(f&quot;Fetched {data}: {elapsed:.3f}s&quot;)
    
    print(f&quot;\nMean fetch time: {timer.mean:.3f}s&quot;)

# run the async code
asyncio.run(main())</code></pre>
        </div>
    </details>
    <h2>Full API Performance Monitor Script</h2>
    <pre><code class="language-python">from suitkaise import timing
import threading
import hashlib
from dataclasses import dataclass
from typing import Dict, Optional

# ──────────────────────────────────────────────────────────────────────────────
# Full API Performance Monitor
#
# A complete system for monitoring API endpoint performance.
# Features:
# - Per-endpoint timing with separate statistics
# - Combined overall statistics
# - Thread-safe for concurrent requests
# - Rolling window to bound memory usage
# - Periodic reporting
# ──────────────────────────────────────────────────────────────────────────────


@dataclass
class EndpointStats:
    &quot;&quot;&quot;Statistics for a single endpoint.&quot;&quot;&quot;
    name: str
    timer: timing.Sktimer
    
    def report(self) -&gt; str:
        &quot;&quot;&quot;Generate a report string for this endpoint.&quot;&quot;&quot;
        if self.timer.num_times == 0:
            return f&quot;{self.name}: no data&quot;
        
        return (f&quot;{self.name}: &quot;
                f&quot;n={self.timer.num_times}, &quot;
                f&quot;mean={self.timer.mean*1000:.1f}ms, &quot;
                f&quot;p95={self.timer.percentile(95)*1000:.1f}ms, &quot;
                f&quot;max={self.timer.max*1000:.1f}ms&quot;)


class APIMonitor:
    &quot;&quot;&quot;Monitor API endpoint performance.&quot;&quot;&quot;
    
    def __init__(self, max_measurements: int = 1000):
        # overall timer for all endpoints
        # - tracks total API performance
        self.overall_timer = timing.Sktimer(max_times=max_measurements)
        
        # per-endpoint timers
        # - allows drilling down into specific endpoint performance
        self._endpoints: Dict[str, EndpointStats] = {}
        self._lock = threading.RLock()
    
    def _get_endpoint(self, name: str) -&gt; EndpointStats:
        &quot;&quot;&quot;Get or create stats for an endpoint.&quot;&quot;&quot;
        with self._lock:
            if name not in self._endpoints:
                # create new timer for this endpoint
                # - same max_times as overall to keep memory bounded
                self._endpoints[name] = EndpointStats(
                    name=name,
                    timer=timing.Sktimer(max_times=1000)
                )
            return self._endpoints[name]
    
    def time_request(self, endpoint: str):
        &quot;&quot;&quot;Context manager for timing a request to an endpoint.&quot;&quot;&quot;
        # get the endpoint&#x27;s timer
        endpoint_stats = self._get_endpoint(endpoint)
        
        # create a TimeThis that records to both timers
        class DualTimer:
            def __init__(self, overall, endpoint):
                self.overall = overall
                self.endpoint = endpoint
                
            def __enter__(self):
                self.overall.start()
                self.endpoint.start()
                return self
            
            def __exit__(self, *args):
                self.overall.stop()
                self.endpoint.stop()
        
        return DualTimer(self.overall_timer, endpoint_stats.timer)
    
    def report(self) -&gt; str:
        &quot;&quot;&quot;Generate a full performance report.&quot;&quot;&quot;
        lines = [&quot;=== API Performance Report ===&quot;, &quot;&quot;]
        
        # overall statistics
        overall = self.overall_timer
        if overall.num_times &gt; 0:
            lines.append(f&quot;Overall: {overall.num_times} requests, &quot;
                        f&quot;mean={overall.mean*1000:.1f}ms, &quot;
                        f&quot;p95={overall.percentile(95)*1000:.1f}ms&quot;)
            lines.append(&quot;&quot;)
        
        # per-endpoint statistics
        lines.append(&quot;Per-endpoint:&quot;)
        with self._lock:
            for stats in sorted(self._endpoints.values(), 
                              key=lambda s: s.timer.mean or 0, 
                              reverse=True):
                lines.append(f&quot;  {stats.report()}&quot;)
        
        return &quot;\n&quot;.join(lines)


# ──────────────────────────────────────────────────────────────────────────────
# API-like Workload
# ──────────────────────────────────────────────────────────────────────────────

def run_api_calls(monitor: APIMonitor, num_calls: int):
    &quot;&quot;&quot;Run deterministic, real work for API-like calls.&quot;&quot;&quot;
    endpoints = [&quot;/users&quot;, &quot;/posts&quot;, &quot;/comments&quot;, &quot;/search&quot;, &quot;/health&quot;]
    payloads = {
        &quot;/users&quot;: b&quot;user\n&quot; * 2000,
        &quot;/posts&quot;: b&quot;post\n&quot; * 4000,
        &quot;/comments&quot;: b&quot;comment\n&quot; * 8000,
        &quot;/search&quot;: b&quot;search\n&quot; * 20000,
        &quot;/health&quot;: b&quot;ok\n&quot; * 200,
    }
    
    for i in range(num_calls):
        endpoint = endpoints[i % len(endpoints)]
        with monitor.time_request(endpoint):
            hashlib.sha256(payloads[endpoint]).digest()


def worker(monitor: APIMonitor, worker_id: int, num_calls: int):
    &quot;&quot;&quot;Worker thread that makes API calls.&quot;&quot;&quot;
    print(f&quot;Worker {worker_id} starting {num_calls} calls...&quot;)
    run_api_calls(monitor, num_calls)
    print(f&quot;Worker {worker_id} completed&quot;)


# create the monitor
# - max_measurements=1000 keeps memory bounded
monitor = APIMonitor(max_measurements=1000)

# spawn multiple worker threads
# - runs concurrent API-like work
threads = []
for i in range(4):
    t = threading.Thread(target=worker, args=(monitor, i, 50))
    threads.append(t)
    t.start()

# wait for all workers
for t in threads:
    t.join()

# print the final report
print(&quot;\n&quot; + monitor.report())


# ──────────────────────────────────────────────────────────────────────────────
# Expected output (times will vary):
# 
# Worker 0 starting 50 calls...
# Worker 1 starting 50 calls...
# Worker 2 starting 50 calls...
# Worker 3 starting 50 calls...
# Worker 0 completed
# Worker 1 completed
# Worker 2 completed
# Worker 3 completed
#
# === API Performance Report ===
# 
# Overall: 200 requests, mean=35.2ms, p95=120.5ms
# 
# Per-endpoint:
#   /search: n=45, mean=98.5ms, p95=142.3ms, max=149.8ms
#   /users: n=38, mean=34.2ms, p95=48.7ms, max=49.9ms
#   /posts: n=42, mean=19.8ms, p95=28.9ms, max=29.8ms
#   /comments: n=35, mean=14.5ms, p95=19.2ms, max=19.9ms
#   /health: n=40, mean=2.8ms, p95=4.8ms, max=4.9ms
# ──────────────────────────────────────────────────────────────────────────────</code></pre>
</section>
