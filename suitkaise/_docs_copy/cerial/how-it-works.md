# How `cerial` actually works

`cerial` converts Python objects into an intermediate representation (IR) that can be serialized by `pickle` into bytes.

Then, after the data is deserialized from bytes back to the IR, `cerial` can take the IR and reconstruct the original Python objects.

```
Object -> Cerializer._serialize_recursive() -> IR -> pickle.dumps() -> bytes
```

```
bytes -> pickle.loads() -> IR -> Decerializer._reconstruct_recursive() -> Object
```

2 central classes use specialized handlers to convert objects that `pickle` cannot handle into the IR format so that they can be serialized.

These same handlers are used to reconstruct the original Python objects from the IR.

There are handlers for a wide variety of objects, which is what allows `cerial` to work with more objects than base `pickle`, `cloudpickle`, and `dill`. 

The aforementioned classes, `Cerializer` and `Decerializer`, handle recursion (walking through nested objects and collections), circular references, and metadata, using the handlers when they come across complex objects (or their IR state).

This allows `cerial` to handle basically any object, including user defined classes.

## Intermediate Representation (IR)

The IR is a nested structure of `pickle` native values. It attaches metadata to the objects so that `cerial` knows how to reconstruct them on the other end.

There are a couple of different things you might see in an IR.

### IR generated by a handler

This is the most common thing you will see in an object's IR.

```python
# lock object's IR
{
    "__cerial_type__": "lock",
    "__handler__": "LockHandler",
    "__object_id__": 140234567890,
    "state": {
        "locked": False
    }
}
```
### `pickle` native wrapper IR (when `__object_id__` is required)

```python
{
    "__cerial_type__": "pickle_native",
    "__object_id__": 123456,
    "value": obj
}
```

### `pickle` native function wrapper IR

```python
{
    "__cerial_type__": "pickle_native_func",
    "__object_id__": 123456,
    "value": obj
}
```

### Circular reference marker

These are used to mark circular references in the IR.

```python
{"__cerial_ref__": 140234567890}
```

### Wrapped collections (for circular-capable containers)

These are used to wrap collections that are capable of handling circular references.

```python
{
    "__cerial_type__": "dict",
    "items": [(k1, v1), (k2, v2)],
    "__object_id__": 123456
}
```

### Simple instance fast-path IR

For simple instances that don't need to pass through the standard flow in order to be serialized, `cerial` will use a fast path to serialize them.

```python
{
    "__cerial_type__": "simple_class_instance",
    "__object_id__": 123,
    "module": "mymodule",
    "qualname": "MyClass",
    "attrs": {"x": 1, "y": 2}
}
```

This is a compact IR format that skips the overhead of the standard flow. It does this by storing a direct reference to the class, and the attributes of a given instance. It still attaches a `__cerial_type__` and `__object_id__` to identify that the object took the fast path and to handle possible circular references.

## Serialization

Serialization is done by a central, internal `Cerializer` class, that uses the handlers to deconstruct complex objects into a nested dictionary of native `pickle` types, which are then serialized to bytes by `pickle.dumps()`.

- converts objects to IR format
- tracks circular references
- enforces maximum recursion depth of 1000
- selects the right handler for a type
- provides the fast paths for common cases
- preserves debug context (object path, depth)
- returns the bytes

### State tracking

`seen_objects: Dict[int, Any]`

Tracks object IDs that were already serialized to detect circular references.

`_serialization_depth: int`

Recursive depth counter; used to prevent runaway recursion. If recursion depth exceeds 1000, a `SerializationError` is raised.

`_object_path: List[str]`

Breadcrumb path to the current object, for error reporting.

`_handler_cache: Dict[type, Handler]`

This is a cache for types that have been processed using a certain handler, so that future objects of that same type can find a valid handler without having to search through `ALL_HANDLERS` again.

### Methods

#### `serialize(obj) -> bytes`

1. Resets internal state.
2. Calls `_serialize_recursive(obj)` to build IR.
3. Pickles IR to bytes using `pickle.dumps()`.
4. Returns the bytes.

#### `serialize_ir(obj) -> Any`

This does the same thing as `serialize`, but returns the IR directly instead of converting it to bytes.

1. Resets internal state.
2. Calls `_serialize_recursive(obj)` to build IR.
3. Returns the IR.

#### `_serialize_recursive(obj) -> Any`

This is the core method that builds the IR for a given object. The steps are ordered in a way that maximizes speed and avoids unnecessary handler calls.

1. Depth check
If `_serialization_depth` exceeds 1000, a `SerializationError` is raised.

2. Primitive fast path
Returns directly for `None`, `bool`, `int`, `float`, `str`, `bytes`.

3. Circular reference check
For objects that can be part of cycles, check `seen_objects` and emit `{"__cerial_ref__": id}`.

4. Check for `pickle` native objects
For common `pickle` native objects, skip handlers and wrap as a `pickle_native` or `pickle_native_func` IR node when needed.

5. simple instance fast path
If the object is:

- a user defined class
- has only primitive attributes
- has no custom serialization

then use the `simple_class_instance` IR instead of a full handler.

6. `pickle` native function fast path
Module level functions without closures are serialized by reference (`module` + `qualname`).

7. Look for a handler
Iterate through `ALL_HANDLERS` (with caching) and find the first `can_handle(obj)` that returns true.

8. Extract state and recurse
`handler.extract_state(obj)` returns a dict. That dict is then recursively serialized by the `_serialize_recursive` method, so on until the state is fully serialized.

9. Wrap into handler IR
The final IR node for the actual object includes `__cerial_type__`, `__handler__`, `__object_id__`, and the serialized `state`.

### Simple instance fast path

The serializer skips the entire handler system for simple instances to reduce overhead.

What counts as a simple instance?

- class is module level (no `<locals>` in `__qualname__` and not nested)
- no `__slots__`
- no custom `__serialize__`
- all attributes in `__dict__` are primitives

When using the fast path, the serializer makes a compact IR that contains `module`, `qualname`, and a direct `attrs` dict with primitives only.

### Function fast path

For module level functions without closures:

1. The serializer records `module` and `qualname`.
2. The deserializer can then import the module and resolve the function by name.

If the function is a lambda, local function, or has closures, `cerial` falls back to the `FunctionHandler` and serializes code objects, globals, and closure state.

(A closure is a function that uses variables from outside itself, like a nested function using a variable from the outer function. The outside values need to be saved too.)

### Handlers

All handlers are defined in `suitkaise/cerial/_int/handlers/`.

They are placed into an `ALL_HANDLERS` list.

- `ALL_HANDLERS` is an ordered list.
- the most specific handlers are earlier in the list
- `ClassInstanceHandler` is last, acting as a catch all for generic or user defined classes
- The serializer caches handler lookups by type, which speeds repeated serialization of the same types.

## Deserialization

Deserialization is done by a central, internal `Decerializer` class, that uses the handlers to reconstruct the objects from the IR.

- converts IR back to live objects
- resolves circular references
- restores handler-based objects
- rebuilds `pickle` native and fast-path types

### State tracking

`_object_registry: Dict[int, Any]`

Maps `__object_id__` to placeholders or reconstructed objects.

`_reconstruction_path: List[str]`

Breadcrumbs for error reporting.

`_reconstruction_depth: int`

Prevents infinite recursion.

`_reconstructing: Set[int]` and `_reconstructed_cache: Dict[int, Any]`

Protect against `pickle` level deduplication of shared IR objects.

### Reconstruction flow

The deserializer uses a two-pass approach to reconstruct the objects from the IR, in order to correctly handle all possible circular references.

1. Placeholder registration
`_register_all_placeholders()` scans the IR and creates empty containers or placeholder objects for each `__object_id__`.

2. Actual reconstruction
`_reconstruct_recursive()` walks the IR and resolves references, using handlers when needed.

### `_reconstruct_recursive(ir)`

1. Deduplicate cache
`pickle` attemps to keep objects the same after loading them from bytes.

If the IR has one collection that is used in 2 or more places, `pickle` won't make 2 copies, instead creating a shared reference to that single copy.

If we don't deduplicate the cache, shared references become lost and multiple different objects are created.

2. Circular reference marker (`__cerial_ref__`)
If the IR node is just a reference, the deserializer looks up the real object in the registry and returns that instead of rebuilding it.

3. Primitive values
Things like `None`, `bool`, `int`, `float`, `str`, `bytes` are already complete. They are returned directly with no reconstruction work.

4. `pickle` native and wrapped collections
Collections might be wrapped (`{"__cerial_type__": "list" ...}`) to preserve identity, or they might be stored as plain lists, tuples, or sets. 

The deserializer rebuilds the container and recursively reconstructs each item in the collection.

5. Handler IRs (`__cerial_type__`)
If the node has a `__cerial_type__`, it belongs to a handler. In this case, we use the matching handler to reconstruct the object using `_reconstruct_from_handler()`.

6. Fallback
If nothing matches, the deserializer returns the IR node as-is. This keeps unknown structures intact instead of just erroring out.

### `_reconstruct_from_handler(data)`

1. Extract data
`type_name`, `handler_name`, `obj_id`, `state` are extracted from the IR node.

These fields tell the deserializer:
- what kind of object it is
- which handler knows how to rebuild it
- the unique object id
- provide the serialized state

2. Find handler
The handler list is searched for a handler whose class name matches `handler_name`.

3. `obj_id` placeholders
Reuse placeholders for `obj_id` if they exist. 

Placeholders were created in the first pass to handle the circular references. Reusing them preserves shared references and cycles.

4. Reconstruct state (recursively)
The handler's `state` may contain nested objects, so the deserializer fully reconstructs that state first before moving on.

5. Call `handler.reconstruct(state)`
The handler uses the reconstructed state to create the real live object.

6. Replace placeholder with real object
If a placeholder was registered, it is swapped out for the real object so all references point to the final live object.

## Handlers

Handlers are helpers that know how to serialize and reconstruct specific object types.

They follow a simple pattern. 
- What they can handle
- How to extract state of an object to turn into IR
- How to reconstruct the object from state

```python
class Handler(ABC):

    type_name: str

    def can_handle(self, obj) -> bool: ...

    def extract_state(self, obj) -> Dict[str, Any]: ...

    def reconstruct(self, state: Dict[str, Any]) -> Any: ...
```

All handlers are defined in `suitkaise/cerial/_int/handlers/` and are registered in `handlers/__init__.py`.

### List of handlers

Handlers are tried in this order.

- `FunctionHandler`: module-level functions (by reference or state)
- `LambdaHandler`: lambda functions with code + closure
- `LoggerHandler`: `logging.Logger` instances
- `StreamHandlerHandler`: `logging.StreamHandler`
- `FileHandlerHandler`: `logging.FileHandler`
- `FormatterHandler`: `logging.Formatter`
- `PartialFunctionHandler`: `functools.partial`
- `BoundMethodHandler`: bound instance methods
- `FileHandleHandler`: open file objects
- `TemporaryFileHandler`: temporary file wrappers
- `StringIOHandler`: `io.StringIO`
- `BytesIOHandler`: `io.BytesIO`
- `LockHandler`: locks and mutexes
- `QueueHandler`: `queue.Queue`
- `MultiprocessingQueueHandler`: `multiprocessing.Queue`
- `HTTPSessionHandler`: HTTP session objects (requests)
- `EventHandler`: `threading.Event`
- `MultiprocessingEventHandler`: `multiprocessing.Event`
- `GeneratorHandler`: generator objects and state
- `RegexPatternHandler`: `re.Pattern`
- `MatchObjectHandler`: `re.Match`
- `SQLiteConnectionHandler`: `sqlite3.Connection`
- `SQLiteCursorHandler`: `sqlite3.Cursor`
- `ContextVarHandler`: `contextvars.ContextVar`
- `TokenHandler`: `contextvars.Token`
- `SocketHandler`: `socket.socket`
- `SemaphoreHandler`: semaphores and bounded semaphores
- `BarrierHandler`: thread barriers
- `ConditionHandler`: condition variables
- `WeakrefHandler`: `weakref.ref`
- `WeakValueDictionaryHandler`: `weakref.WeakValueDictionary`
- `WeakKeyDictionaryHandler`: `weakref.WeakKeyDictionary`
- `IteratorHandler`: generic iterators
- `RangeHandler`: `range`
- `EnumerateHandler`: `enumerate`
- `ZipHandler`: `zip`
- `MMapHandler`: `mmap.mmap`
- `ThreadHandler`: `threading.Thread`
- `ThreadPoolExecutorHandler`: thread pool executors
- `ProcessPoolExecutorHandler`: process pool executors
- `OSPipeHandler`: OS pipe file objects
- `MultiprocessingPipeHandler`: `multiprocessing.Pipe` connections
- `MultiprocessingManagerHandler`: manager/proxy objects
- `SharedMemoryHandler`: `multiprocessing.shared_memory.SharedMemory`
- `MemoryViewHandler`: `memoryview`
- `FileDescriptorHandler`: raw file descriptor ints
- `CodeObjectHandler`: code objects
- `FrameObjectHandler`: frame objects
- `PropertyHandler`: `property` descriptors
- `DescriptorHandler`: low-level descriptors
- `ThreadLocalHandler`: `threading.local`
- `StaticMethodHandler`: `staticmethod` wrappers
- `ClassMethodHandler`: `classmethod` wrappers
- `EnumHandler`: enum instances
- `EnumClassHandler`: enum classes
- `NamedTupleHandler`: namedtuple instances
- `TypedDictHandler`: TypedDict objects
- `ContextManagerHandler`: user-defined context managers
- `ContextlibGeneratorHandler`: `contextlib` generator managers
- `PopenHandler`: `subprocess.Popen`
- `CompletedProcessHandler`: `subprocess.CompletedProcess`
- `CoroutineHandler`: coroutine objects
- `AsyncGeneratorHandler`: async generator objects
- `TaskHandler`: `asyncio.Task`
- `FutureHandler`: `asyncio.Future`
- `ModuleHandler`: module objects
- `DatabaseConnectionHandler`: database connection objects (generic)
- `ClassObjectHandler`: class objects
- `ClassInstanceHandler`: class instances (catch-all)

## Reconnectors

Network sockets, database connections, threads, subprocesses, and more cannot be serialized safely due to different reasons.

Additionally, auto reconnecting live resources like these can lead to unexpected behavior.

For these cases, we return `Reconnector` instances that store as much metadata as possible to recreate the live resource, acting as a placeholder until we actually reconnect the live resource.

1. Original object is serialized
2. Object is then deserialized (likely in a different process)
3. `Reconnector` is constructed from IR state and returned
4. `Reconnector.reconnect()` is called to create a new live resource (may require authentication)

How it works:
- `cerial` creates a placeholder `Reconnector` object
- You call `reconnect()` to create the new live resource, providing any authentication needed

### `reconnect_all(obj, **auth)`

`cerial` also includes a function called `reconnect_all()`.

Args:
- `start_threads`: if True, any `threading.Thread` objects returned by reconnectors are automatically started after reconnect. This is a keyword only argument.
- `**auth`: a mapping of type key to secrets (authentication).

#### `start_threads`

If `start_threads=True`, any `threading.Thread` objects returned by reconnectors are automatically started after reconnect.

This allows you to quickly reconstruct all live resources in a given object.

#### `**auth`

`**auth` is a mapping of type key to secrets (authentication).

Type keys are strings like `"psycopg2.Connection"` or `"redis.Redis"`, that are the actual connection types.

Each type key maps to a dict where:

- `"*"` is the default auth for all instances of that type
- Specific attribute names override the default for that attribute

```python
auth = {
    "psycopg2.Connection": {
        "*": "default_psycopg2_password",
        "analytics_db": "analytics_password"  # used for obj.analytics_db specifically
    },
    "redis.Redis": {
        "*": "your_redis_password"
    }
}
```

1. Determine the type key from the reconnector
2. If the object is stored in a dict and the key is a string, treat that key as the attribute name
3. If there is a matching entry for that attribute name, use it
4. Otherwise, fall back to `"*"` for that type
5. If no auth is found, `reconnect()` is called with no credentials

### `DbReconnector`

Database connections can't be pickled directly, and many require additional credentials (like a password or API key) to connect.

Authentication credentials should not be serialized into an IR for security reasons.

Instead, we store things like the host, port, user, database name, and everything else so that `reconnect()` can quickly recreate the same connection to your database. All you need to do is pass `auth` so that `reconnect()` can use the correct credentials.

`DbReconnector` actually has many specialized subclasses for different database types. 

`auth` for each type is their respective password, token, or other authentication needed in order to create a connection to the actual `*.Connection` object.

For example, `password` is the arg for `psycopg2.connect()`. `auth` for `PostgresReconnector` will input what you pass as the `password` arg.

#### List of `DbReconnector` subclasses

- `PostgresReconnector`
types: `psycopg2.Connection`, `psycopg.Connection`
auth: required
data collected:
    - `host`
    - `port`
    - `user`
    - `database`
    - `dsn` / `url` (if available)
result: `psycopg2.Connection` or `psycopg.Connection`
limitations: not the same session; open transactions, cursors, and server state are not preserved

- `MySQLReconnector`
types: `pymysql.Connection`, `mysql.connector.connect()`, `mariadb.Connection`
auth: required
data collected:
    - `host`
    - `port`
    - `user`
    - `database`
result: `pymysql.Connection` / `mysql.connector.connection_cext.CMySQLConnection` / `mariadb.Connection`
limitations: not the same session; open transactions and server state are not preserved

- `SQLiteReconnector`
types: `sqlite3.Connection`
auth: NOT required (path only)
data collected:
    - `path` / `database`
result: `sqlite3.Connection`
limitations: in-memory databases do not persist across processes; file locks may force fallback to `:memory:`

You still need to call `reconnect()` or `reconnect_all()` to create the actual `sqlite3.Connection` object again, but we capture all data needed to recreate the object.

- `MongoReconnector`
types: `pymongo.MongoClient`
auth: required
data collected:
    - `uri` / `url` (if available)
    - `host`
    - `port`
    - `username` / `user`
    - `authSource` / `auth_source`
result: `pymongo.MongoClient`
limitations: not the same server session; any in-progress operations are lost

- `SQLAlchemyReconnector`
types: `sqlalchemy.Engine`, `sqlalchemy.Connection`
auth: required
data collected:
    - `url` / `uri` / `dsn` (if available)
    - `driver` / `drivername`
    - `host`
    - `port`
    - `database`
    - `user` / `username`
    - `query` (if present)
result: `sqlalchemy.engine.Connection` (via `Engine.connect()`)
limitations: does not preserve engine pool state or active transactions

- `CassandraReconnector`
types: `cassandra.cluster.Cluster`
auth: required
data collected:
    - `contact_points` / `hosts` / `nodes`
    - `port`
    - `username` / `user`
    - `keyspace` (if present)
result: `cassandra.cluster.Session`
limitations: cluster/session state is new; in-flight queries are not preserved

- `ElasticsearchReconnector`
types: `elasticsearch.Elasticsearch`
auth: required (api_key supported)
data collected:
    - `hosts` or `url` / `uri`
    - `user` / `username`
result: `elasticsearch.Elasticsearch`
limitations: no preserved connections or request state

- `Neo4jReconnector`
types: `neo4j.Driver`
auth: required
data collected:
    - `uri` / `url`
    - `host`
    - `port`
    - `scheme`
    - `user` / `username`
    - `encrypted` (if present)
result: `neo4j.Driver`
limitations: existing sessions and transactions are not preserved

Api key is also supported.

- `InfluxDBReconnector`
types: `influxdb.InfluxDBClient`, `influxdb_client.InfluxDBClient`
auth: required (`token` for v2)
data collected:
    - `url` / `uri`
    - `host`
    - `port`
    - `user`
    - `database`
    - `org` (v2)
    - `timeout` (if present)
    - `verify_ssl` (if present)
result: `influxdb.InfluxDBClient` or `influxdb_client.InfluxDBClient`
limitations: server-side session state is not preserved

- `ODBCReconnector`
types: `pyodbc.Connection`
auth: required
data collected:
    - `dsn`
    - `driver`
    - `server` / `host`
    - `port`
    - `database` / `db`
    - `user` / `username` / `uid`
result: `pyodbc.Connection`
limitations: any active transactions/cursors are lost

- `ClickHouseReconnector`
types: `clickhouse_driver.Client`
auth: required
data collected:
    - `host`
    - `port`
    - `user`
    - `database`
result: `clickhouse_driver.Client`
limitations: no preserved session state

- `MSSQLReconnector`
types: `pymssql.Connection`
auth: required
data collected:
    - `host`
    - `port`
    - `user`
    - `database`
result: `pymssql.Connection`
limitations: open transactions and session state are not preserved

- `OracleReconnector`
types: `oracledb.Connection`, `cx_Oracle.Connection`
auth: required
data collected:
    - `dsn`
    - `host`
    - `port`
    - `service_name` / `database`
    - `user`
result: `oracledb.Connection` or `cx_Oracle.Connection`
limitations: active sessions/transactions are not preserved

- `SnowflakeReconnector`
types: `snowflake.connector.Connection`
auth: required
data collected:
    - `user`
    - `account`
    - `warehouse`
    - `database`
    - `schema`
    - `role`
result: `snowflake.connector.Connection`
limitations: session state (role, warehouse changes) is re-established from stored params only

- `DuckDBReconnector`
types: `duckdb.Connection`
auth: NOT required (path only)
data collected:
    - `path` / `database`
result: `duckdb.Connection`
limitations: in-memory databases do not persist across processes

You still need to call `reconnect()` or `reconnect_all()` to create the actual `duckdb.Connection` object again, but we capture all data needed to recreate the object.


### `SocketReconnector` --> `socket.socket`

Live sockets are OS resources that can't be serialized safely.

Data collected:
- `family`
- `type`
- `proto`
- `timeout`
- `blocking`
- `local_addr`
- `remote_addr`

When you call `reconnect()`, the socket is created with the same parameters and reconnects/binds as appropriate.

Result: `socket.socket` (fresh socket)

Limitations:
- any in-flight data is lost
- the new socket is not the exact same connection instance

### `ThreadReconnector` --> `threading.Thread`

Threads run in their own process's memory space. You cannot directly serialize and reconnect to the exact same thread in the same memory space if the thread object has moved to a different process.

However, it is still useful to quickly recreate the exact same thread in a different process. This is why `cerial` includes a `ThreadReconnector`.

If an object is bouncing around different places in memory (running in different processes with different GILs), it is useful and convenient to be able to quickly start something like a background runner thread or a monitoring thread without having to manually create the thread object and start it.

It is also useful when doing something like adding a `ThreadReconnector` to shared memory using `Share` or `Skprocess.tell()` to quickly start a thread in multiple different places.

Data collected:
- `name`
- `daemon`
- `target`
- `args`
- `kwargs`
- `is_alive` (informational only)

When you call `reconnect()`, a new `threading.Thread` object is constructed with the same exact configuration.

Result: `threading.Thread` (not started)

`cerial` does not start the thread by default because that could cause silent issues or unexpected behavior. 

If you want to start the thread automatically when you reconnect, use:

```python
thread = reconnector.reconnect(start=True)

# reconnect_all()
reconnect_all(obj, start_threads=True, **auth)

# autoreconnect decorator on Skprocess class
from suitkaise.processing import Skprocess, autoreconnect
@autoreconnect(start_threads=True, **auth)
class MyProcess(Skprocess):

    # ...
```

Limitations:
- original execution state is not preserved

### `PipeReconnector` --> `multiprocessing.Pipe` / OS pipes

Created from OS pipe file objects and `multiprocessing.Pipe` endpoints.

Pipes are OS level handles tied to a specific process. The file descriptors or connection handles are not valid outside the original process boundary unless one side of the pipe is explicitly passed to a new process as it is being created.

Since pipes are actually OS handles, they cannot be converted down to bytes and then passed. When going through `cerial` without using the `Skprocess` class, OS handles get lost because of this, meaning pipes won't actually work correctly.

If you want to use pipes with `cerial`, use the `Pipe` class from `suitkaise.processing` instead of relying on the `PipeReconnector`.

Data collected:
- `readable`
- `writable`
- `closed`
- `duplex`
- `preferred_end`

Reconnector specific attributes:
- `has_endpoint` (True if the original pipe was strictly read-only or write-only)
- `endpoint` (`"read"` or `"write"` when `has_endpoint` is True)

`PipeReconnector` has 3 methods instead of just `reconnect()`:

- `reconnect()` -> returns one end of a new pipe
- `peer()` -> returns the other end of the pipe
- `pair()` -> returns both ends of a new pipe, ready for use

These are new `multiprocessing.connection.Connection` objects, that do not point to the original parent, but are still valid and ready to use.

NOTE: `PipeReconnector` does not apply to `suitkaise.processing.Pipe` objects. These are handled directly using `__serialize__` and `__deserialize__` and preserve pipe handles.

Limitations:
- buffered data in the old pipe is lost
- the new pipe is not connected to the existing peer process it came from

### `SubprocessReconnector` --> `subprocess.Popen`

Subprocesses are OS processes and cannot be paused and moved between processes. We store the command args and output metadata so you can restart it if needed, but the actual process and its runtime state are not preserved.

Data collected:
- `args`
- `returncode`
- `pid` (original)
- `poll_result`
- `stdout_data`
- `stderr_data`

When you call `reconnect()`, a new subprocess is started with the saved launch parameters (args and other state).

Result: `subprocess.Popen` (new process)

Limitations:
- original PID and runtime state are not preserved
- if the original process was running, it is not resumed, it is restarted

### `MatchReconnector` --> `re.Match`

`re.Match` objects are not constructible directly because Python doesnâ€™t expose a public constructor for them. They only exist as the result of running a compiled regex pattern against a specific string. To recreate one, we store all of the data needed to recreate the match.

When you call `reconnect()`, the pattern is recompiled and re-run on the same string, and then groups and span info are restored. If the match is identical, a new `re.Match` is returned. `None` is returned if it is not.

Data collected:
- `pattern`
- `flags`
- `string`
- `pos`
- `endpos`
- `match_string`
- `span`
- `groups`
- `groupdict`

Result: `re.Match` (if the match can be reproduced)

Limitations:
- if the input string or flags differ, the match may not reproduce.

### Limitations

As seen above, some limitations are present.

This is due to how Python handles these resources when they cross process boundaries, and something that `cerial` has to find workarounds for.

Using `Reconnectors` is the best way we can work with Python's architecture to allow you to use this in a cross-process/distributed environment.

It also ensures that even if you have these object types in your object, you won't receive errors when serializing/deserializing it.

Why not automatically reconnect these resources?

2 reasons: security and user expectations.

#### Security

In order to automatically reconnect objects like database connections that require authentication, we would need to pass that authentication somewhere in the IR. This data is easily readable and can easily be compromised.

In general, it is not good practice to include sensitive data when moving objects between processes. `cerial` does everything else for you so all you have to to is reaccess and add the authentication once your object reaches the target process.

#### User expectations

Automatically reconnecting things like this is awkward for users.

- connections normally don't reconnect automatically
- silently reconnecting things can cause silent failures or user confusion
- users expect modules/tools like these to adhere to good practices when it comes to security and authentication

`suitkaise` offers 3 levels of user control regarding reconnection.

1. (most manual control) calling `reconnect()` on each `Reconnector` object, adding each `auth` individually
2. calling `reconnect_all(obj, **auth)` on an entire object
3. (most automatic) using `@autoreconnect(start_threads=True, **auth)` to decorate a `Skprocess` class

We can automatically reconnect for you, but it is controlled and requires you to provide authentication through a decorator and use a specific class.





